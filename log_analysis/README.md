## Logs
### Loss
- Start Loss : 235
- Epoch-1 Loss : 66
- Epoch-2 Loss : 39
- Epoch-3 Loss : 28
- Epoch-4 Loss : 24
- Epoch-5 Loss : 20
- Epoch-6 Loss : 19
- Epoch-7 Loss : 19
- Epoch-8 Loss : 17
- Epoch-9 Loss : 15
- Epoch-10 Loss : 15
- Epoch-11 ~ 27 Loss : 13 ~ 14

### Evaluation Result
- Without Pretrained Model

| ID | Time | Name                         | Epoch | Loss | Best mAP       | Purpose                         | Analysis              | Notes  |
|:--:|:----:|:----------------------------:|:-----:|:----:|:--------------:|:-------------------------------:|:---------------------:|:------:|
| 01 | 0525 | 不使用预训练模型              | 20    | 17.1 | 0.0515/19      | 观察直接训练的效果               | 直接训练提升很慢       | |
| 02 | 0524 | 预训练　+ 不冻结              | 13    | 9.48 | 2.35/3         | 比较不同冻结方案的差异           | 直接使用预训练不可取　  | |
| 03 | 0525 | 预训练　+ 冻结backbone        | 50    | 1.66 | 22.87/46       | 比较不同冻结方案的差异           | 有一定提升             | |
| 04 | 0525 | 预训练　+ 除三个检测层都冻结   | 28    | 12.6 | 29.9/23        | 比较不同冻结方案的差异           | 这是最好的冻结方案     | |
| 05 | 0526 | 同04 + 关闭多尺度训练　　　 　 | 27    | 12.9 | 27.7/12        | 观察多尺度训练带来的提升         | 有一定提升，但并不明显 | |
| 06 | 0526 | 同04　+ lr_1 = 1e-4           | 40    | 17.7 | 26.7/40        | 调低学习率，实验能否突破原有成绩 | 无法提升原有成绩       | |
| 07 | 0526 | 04最好的模型之上微调           | 12    | 2.85 | 27.74/10       | 观察微调最好模型能否突破原有成绩 | 无法提升原有成绩       | |
| 08 | 0526 | 同04(20轮) + 20轮微调　　　　　| 36    | 11.9 | 27.38/29　     | 实验冻结预训练+解除冻结开始微调  | 无法观察到效果提升     | |
| 08 | 0526 | 同04 + dataset_anchors        | 27    | 12.9 | 27.65/23       | 使用数据集宽高,观察能否带来提升  | 无法观察到效果提升     | 初始map有一定提升 |
